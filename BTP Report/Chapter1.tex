\section{The Area of Work}
 
\subsection{What is Big Data\& Why it matters}
Big data is an evolving term that describes any voluminous amount of structured, semi-structured and unstructured data that has the potential to be mined for information. Although big data doesn't refer to any specific quantity, the term is often used when speaking about petabytes and exabytes of data.

Big data can be characterized by 3Vs: the extreme volume of data, the wide variety of types of data and the velocity at which the data must be must processed.

\textbf{Volume} Big data implies enormous volumes of data. It used to be employees created data. Now that data is generated by machines, networks and human interaction on systems like social media the volume of data to be analyzed is massive \textbf{Variety}. Variety refers to the many sources and types of data both structured and unstructured. We used to store data from sources like spreadsheets and databases. Now data comes in the form of emails, photos, videos, monitoring devices, PDFs, audio, etc. This variety of unstructured data creates problems for storage, mining and analyzing data \textbf{Velocity}.    

Big Data Velocity deals with the pace at which data flows in from sources like business processes, machines, networks and human interaction with things like social media sites, mobile devices, etc. The flow of data is massive and continuous. This real-time data can help researchers and businesses make valuable decisions that provide strategic competitive advantages and ROI if you are able to handle the velocity.


\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{Figures/fig-4.jpg}
\caption{}
\label{}
\end{figure}
A number of recent technology advancements enable organizations to make the most of big data and big data analytics:
\vspace*{-2mm}

\begin{description}
\vspace*{-2mm}
\item[1]Cheap, abundant storage.
\vspace*{-2mm}
\item[2]Faster processors.
\vspace*{-2mm}
\item[3]Affordable open source, distributed big data platforms, such as Hadoop.
\end{description}

\subsection{Hadoop}\cite{hadoop}
Apache Hadoop is an open source framework written in java for distributed storage and distributed processing of very large data sets on computer clusters built on commodity hardware. It is software framework for storing and processing big data.  It accomplishes two tasks massive data storage and faster processing.  Hadoop was initially inspired by papers published by Google outlining its approach to handling an avalanche of data, and has since become the de facto standard for storing,    processing and analyzing hundreds of terabytes, and even petabytes of data.  

Instead of relying on expensive, proprietary hardware and different systems to store and process data, Hadoop enables distributed parallel processing of huge amounts of data across inexpensive, industry-standard servers that both store and process the data, and can scale without limits. With Hadoop no data is considered to be big .It is designed up from a single server to thousands of machines. Rather than relying on high-end hardware, the resiliency of these clusters comes from the software's ability to detect and handle failures at the application layer.  And in today’s world where lots of amount of data is being created everyday , Hadoop’s breakthrough advantages mean that businesses and organizations can now find value in data that was recently considered useless. All the modules in hadoop are designed with a hardware  (of individual machines, or racks of machines) are commonplace and thus should be automatically handled in software by the framework.

The core of Apache hadoop consists of a Storage part HDFS(Hadoop Distributed File System) and has a processing part ( Map Reduce) .  Hadoop splits files into large blocks and distributes them amongst the nodes in the cluster. To process the large amount of data hadoop MapReduce transfer codes in the form package to nodes for the parallel processing  based on the data each node needs to process. This approach takes in account of data locality to manipulate the data on hand to allow the data to beprocessed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are connected via high-speed networking. The Apache hadoop software  library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.

The Project includes these modules:
\begin{description}
\item[1. Hadoop Common]:  The common utilities that support the other Hadoop modules i.e  the requirements by the other modules.
\item[2. Hadoop Distributed File System (HDFS)]: It is a distributed file system that stores huge amount of data on commodity machines. A distributed file system that provides  high-throughput access to application data.
\item[3.Hadoop YARN]: It is a resource management platform i.e. a framework for job scheduling and cluster resource management.
\item[4. Hadoop MapReduce]: A YARN-based system for parallel processing of large data sets.

\end{description}
Hadoop was created by Doug Cutting, the creator of Apache Lucene, the widely used text search library. Hadoop has its origins in Apache Nutch, an open source web search engine, itself a part of the Lucene project.
Nutch was started in 2002, and a working crawler and search system quickly emerged. However their architecture wouldn’t scale to the billions of pages on the Web. In 2003 Google published paper on Google’s Distributed Filesystem (GFS) which was being used in production at Google. Hence in 2004 they implemented Nutch Distributed Filesystem  (NDFS) using GFS architecture that would solve their storage needs for very large files generated as a part of the web crawl and indexing process.
In 2004, Google published the paper that introduced MapReduce to the world. NDFS and the MapReduce implementation in Nutch were applicable beyond the realm of search, and in February 2006 they moved out of Nutch to form an independent subproject of Lucene called Hadoop.

\subsubsection{Why use Hadoop?}
Hadoop changes the economics and dynamic scale of large scale computing.
There are following characteristics of Hadoop.
\begin{description}

\item[1. Scalable] A cluster can be expanded by adding new servers or resources without having to move, reformat, or change the dependent analytic workflows or applications.
\item[2. Flexible] Hadoop is schema-less and can absorb any type of data,structured   or not, from any number of sources. Data from multiple sources can be joined and   aggregated in arbitrary ways enabling deeper analyses than any one system can provide i.e different data set can be combined of different formats to give a efficient result.
\item[3. Cost effective] Hadoop brings massively parallel computing to commodity servers. The result is a sizeable decrease in the cost per terabyte of storage, which in turn makes it affordable to model all your data.
\item[4. Fault tolerant]  When you lose a node, the system redirects work to another location of the data and continues processing without missing a beat.
\item[5. Massive storage]. The Hadoop framework can store huge amounts of data by breaking the data into blocks and storing it on clusters of lower-cost commodity hardware.
\item[6. Distributed] Data is divided and stored across multiple computers, and computations can be run in parallel across multiple connected machines.
i.e. collection of large datasets which allows to find out useful information.
\end{description}

\subsubsection{Hadoop Ecosystem}
Hadoop is supplemented by an ecosystem of Apache open-source projects that extend the value of Hadoop and improve its usability. Some of these Apache opensource software projects are: 
\begin{description}

\item[1. Pig]A programming language designed to handle any type of data, helping users to focus more on analyzing large data sets and less on writing map programs and reduce programs. 
\item[2. Hive]A Hadoop runtime component that allows those fluent with SQL to write Hive Query Language (HQL) statements, which are similar to SQL statements. These are broken down into MapReduce jobs and executed across the cluster.
\item[3. Flume]A distributed, reliable and available service for efficiently collecting, aggregating and moving large amounts of log data. Its main goal is to deliver data from applications to the HDFS. 
\item[4. HBase]A column-oriented non-relational (noSQL) database that runs on top of HDFS and is often used for sparse data sets. 
\item[5. Mahout]A Scalable machine learning and data mining library.
\item[6. Spark]A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.
\item[7. Cassandra]A scalable multi-master database with no single points of failure.
\end{description}

\subsubsection{Hadoop Architecture}
Hadoop consists of the Hadoop common package which provides file system and OS level architecture a map reduce engine and a Hadoop distributed file system (HDFS) . The Hadoop Common package contains the necessary Java ARchive (JAR) files and scripts needed to start Hadoop.  

For effective scheduling of the work , every Hadoop compatible file system should provide location awareness the name of the rack where work node is located. The goal is to reduce the impact of a rack power outage or switch failure, so that even if these events occur, the data may still be readable. A small Hadoop cluster has a single master node and multiple worker nodes. The master node includes  a Job Tracker , Task Tracker, Name Node and Data Node. 

A slave or worker node behaves as both a Data Node and Task Tracker, though it is possible to have data-only worker nodes and compute-only worker nodes. These are normally used only in nonstandard applications. Hadoop requirement is Java Runtime Environment  version 1.6 or higher. In a large cluster of data HDFS is controlled by Name Node server to host the file system index, and a secondary Name Node that can generate snapshots of the name node's memory structures. It  prevents file-system corruption and reduces loss of data. Job tracker server manages scheduling of jobs in a server. In cluster Hadoop map reduce engine is being used against an alternating file system the name node, secondary name node, data node architecture of HDFS are updated by file system Specific equivalent.

\subsection{MapReduce}
MapReduce is a programming paradigm for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. The framework is divided into two parts: Map, allows to parcels out work to different nodes in the distributed cluster. Reduce, collates the work and resolves the results into a single value.

MapReduce framework consists of a single master JobTracker and one slave TaskTracker per cluster-node. Master is responsible for scheduling the jobs' component tasks on the slaves, monitoring them and re-executing the failed tasks. Although the Hadoop framework is implemented in Java, MapReduce applications can be written in Python, Ruby, R, C++. Eg. Hadoop Streaming, Hadoop Pipes.

\subsubsection{Hadoop-MapReduce Architecture}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{Figures/fig-5.png}
\caption{}
\label{}
\end{figure}
Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.
A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.
The MapReduce framework consists of a single master JobTracker and one slave TaskTracker per cluster-node. The master is responsible for scheduling the jobs' component tasks on the slaves, monitoring them and re-executing the failed tasks. The slaves execute the tasks as directed by the master.
The MapReduce framework operates exclusively on <key, value> pairs, that is, the framework views the input to the job as a set of <key, value> pairs and produces a set of <key, value> pairs as the output of the job, conceivably of different types.
\begin{description}
\item[Map]: Each worker node applies the "map()" function to the local data, and writes the output to a temporary storage. A master node arrange that for redundant copies of input data, only one is processed.

\item[Shuffle]: Worker nodes redistribute data based on the output keys (produced by the "map()" function), such that all data belonging to one key is located on the same worker node.
\item[Reduce]: Worker nodes now process each group of output data, per key, in parallel.
\end{description}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{Figures/fig-6.png}
\caption{}
\label{}
\end{figure}

\subsubsection{Map Reduce in Amazon Aws EC2} Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and cost-effectively process vast amounts of data. Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and cost-effectively process vast amounts of data.
Amazon EMR simplifies big data processing, providing a managed Hadoop framework that makes it easy, fast, and cost-effective for you to distribute and process vast amounts of your data across dynamically scalable Amazon EC2 instances. The EC2 instances used to run an Elastic MapReduce job flow fall in to one of three categories or instance groups:
\begin{description}
\item[Master]The Master instance group contains a single EC2 instance. This instance schedules Hadoop tasks on the Core and Task nodes.
\item[Core]The Core instance group contains one or more EC2 instances. These instances use HDFS to store the data for the job flow. They also run mapper and reducer tasks as specified in the job flow. This group can be expanded in order to accelerate a running job flow.
\item[Task]The Task instance group contains zero or more EC2 instances and runs mapper and reduce tasks. Since they don’t store any data, this group can expand or contract during the course of a job flow.
\end{description}

\subsubsection{MapReduce MapR}
MapR is a third-party application offering an open, enterprise-grade distribution that makes Hadoop easier to use and more dependable. For ease of use, MapR provides network file system (NFS) and open database connectivity (ODBC) interfaces, a comprehensive management suite, and automatic compression. For dependability, MapR provides high availability with a self-healing no-NameNode architecture, and data protection with snapshots, disaster recovery, and with cross-cluster mirroring.

All three MapR Editions for Hadoop (M7, M5 and M3) are available within the EMR service.
MapR M7 is the fastest and most reliable distribution for Apache Hadoop, and includes an enterprise-grade online database that adds the speed, scalability, and flexibility of NoSQL databases. MapR M7 is the only distribution built for running both operational and analytical workloads in the same cluster.

\subsection{Hadoop Distributions}
These products have been emerged out from Hadoop and incresingly become fast, reliable and scalable.

\subsubsection{Cloudera Hadoop}
Cloudera Inc. was founded by big data geniuses from Facebook, Google, Oracle and Yahoo in 2008. It was the first company to develop and distribute Apache Hadoop-based software and still has the largest user base with most number of clients. Although the core of the distribution is based on Apache Hadoop, it also provides a proprietary Cloudera Management Suite to automate the installation process and provide other services to enhance convenience of users which include reducing deployment time, displaying real time nodes’ count, etc.

\subsubsection{Hortonworks Hadoop}
Hortonworks, founded in 2011, has quickly emerged as one of the leading vendors of Hadoop. The distribution provides open source platform based on Apache Hadoop for analysing, storing and managing big data. Hortonworks is the only commercial vendor to distribute complete open source Apache Hadoop without additional proprietary software. Hortonworks’ distribution HDP2.0 can be directly downloaded from their website free of cost and is easy to install. The engineers of Hortonworks are behind most of Hadoop’s recent innovations including Yarn, which is better than MapReduce in the sense that it will enable inclusion of more data processing frameworks.

\subsubsection{Apache Spark}
Unlike Hadoop which is a batch processing system, it provides Real-Time Analytics.
\begin{description}
\item[1. Fast Analytics and Stream Processing] Apache Spark is an open source, parallel data processing framework that complements Apache Hadoop to make it easy to develop fast, unified Big Data applications combining batch, streaming, and interactive analytics on all your data

\item[2. Fast, Powerful Data Processing] For analysts and data scientists who rely on iterative algorithms (e.g. clustering/classification), Spark is 10-100x faster than MapReduce delivering faster time to insight on more data, resulting in better business decisions and user outcomes. Spark is 
\begin{description}
\item[Fast]: Data processing up to 100x faster than MapReduce, both in-memory and on disk
\item[Powerful]: Write sophisticated parallel applications quickly in Java, Scala, or Python without having to think in terms of only “map” and “reduce” operators
\item[Integrated]: Spark is deeply integrated with CDH, able to read any data in HDFS and deployed through Cloudera Manager
\end{description}
\end{description}

\subsection{Data Analytics and Visualization}
Data analytics (DA) is the science of examining raw data with the purpose of drawing conclusions about that information. Data analytics is used in many industries to allow companies and organization to make better business decisions and in the sciences to verify or disprove existing models or theories. Data analytics is distinguished from data mining by the scope, purpose and focus of the analysis. Data miners sort through huge data sets using sophisticated software to identify undiscovered patterns and establish hidden relationships. Data analytics focuses on inference, the process of deriving a conclusion based solely on what is already known by the researcher.

Data visualization is the presentation of data in a pictorial or graphical format. For centuries, people have depended on visual representations such as charts and maps to understand information more easily and quickly.

As more and more data is collected and analyzed, decision makers at all levels welcome data visualization software that enables them to see analytical results presented visually, find relevance among the millions of variables, communicate concepts and hypotheses to others, and even predict the future.

Because of the way the human brain processes information, it is faster for people to grasp the meaning of many data points when they are displayed in charts and graphs rather than poring over piles of spreadsheets or reading pages and pages of reports.

Interactive data visualization goes a step further – moving beyond the display of static graphics and spreadsheets to using computers and mobile devices to drill down into charts and graphs for more details, and interactively (and immediately) changing what data you see and how it is processed.

\subsubsection{Tools for Data Visualization}
There are abundant visulization tools but most widely used opensource tools are:
\begin{description}
\item[ggplot] ggplot2 is a plotting system for R, based on the grammar of graphics, which tries to take the good parts of base and lattice graphics and none of the bad parts. It takes care of many of the fiddly details that make plotting a hassle (like drawing legends) as well as providing a powerful model of graphics that makes it easy to produce complex multi-layered graphics.
\item[googleVis] googleVis is an R package providing an interface between R and Google Charts. The functions of the package allow the user to visualise data with the Google Chart Tools without uploading their data to Google. The output of googleVis functions is html code that contains the data and references to JavaScript functions hosted by Google. To view the output a browser with Flash and Internet connection is required, the actual chart is rendered in the browser. 
\item[D3] D3 allows you to bind arbitrary data to a Document Object Model (DOM), and then apply data-driven transformations to the document. For example, you can use D3 to generate an HTML table from an array of numbers. Or, use the same data to create an interactive SVG bar chart with smooth transitions and interaction. 
\end{description}
\section{Problem Addressed}
\input{problemAddressed.tex}


\section{Existing System}
\input{existing_system.tex}


\subsection{The  Oracle Airline Data Model} 
The  Oracle  Airline Data Model is a powerful logical and physical data model that will help  airlines effectively store, manage, and analyze airline data that currently resides in passenger service systems (includes
reservation systems and departure control systems), global distribution system (GDS), loyalty management systems, and customer data warehouses.  It provides a single scalable repository for transactional and historical data 
that can be used to provide real-time business intelligence and strategic 
insights you’re  your airline. Using sophisticated trending and data mining capabilities based on Oracle’s  OLAP and data mining technology,  airline personnel will now have the data analysis  capabilities to develop Airline -specific insights that are relevant, actionable, and can improve both top-line and bottom-line results. 

The Oracle Airline Data Model provides detail transaction storage and advanced analysis into a full range of airline
subject areas, including reservations, sales, operations, loyalty, 
and finance.  Using reservation data, the data model can provide detailed insight into passenger bookings by time period, fare class, and flight.  It provides insights into channel performance looking at bookings, cancellations, and revenues through travel agency, OTA, ticket counter, call center, and web channels.  It allows you to analyze passenger 
revenues by geography, time period, and flight.  Finally it provides insights into loyalty program member activity through a variety of reports.  
The data model fits the needs of large network carriers and low-cost carriers. 

 





